import { Meta, Markdown } from '@storybook/blocks';
import Shot from "../assets/shot_lexio_llama_index.png";

<Meta title="Gallery/Base: Llama-Index" />
# ðŸš€ Llama Index Demo

This demo shows the integration of a Llama Index with a FastAPI backend and a React frontend with Lexio UI components.  Llama Index is used to index and query documents.


## Preparation
To run this demo, you need to clone the Lexio repo:
```bash
git@github.com:Renumics/lexio.git
```

You need to build the Lexio UI components:
```bash
cd lexio
npm run build
```
and put some data in the `examples/llama-index/data` folder.

Make sure you have a OpenAI API key set as `OPENAI_API_KEY` environment variable.

Now you can run the demo in the `examples/llama-index` folder. You can run the demo by running the following commands from the examples/llama-index folder:
```bash
make setup-backend
make setup-frontend
make start-backend
make start-frontend
```
This will open a browser window with the demo:
<img src={Shot} alt="Screenshot of the demo" />
<small><i>(Document source: WMO-No. 1360: " State of the Climate in Africa")</i></small>


## Backend Description

The backend is built with FastAPI and uses the Llama Index to manage document indexing and querying. The main components include:

- **Document Indexing with Llama Index**: PDF documents are processed, indexed and a query engine is created with just a few line of code:
```python
index = VectorStoreIndex.from_documents(SimpleDirectoryReader(DATA_FOLDER).load_data())
query_engine = index.as_query_engine()
```

- **Query endpoint with FastAPI**: The query engine is exposed as an endpoint that can be queried by the frontend:
```python
@app.get("/query")
async def retrieve_and_generate(messages: str = Query(...)):
    response = query_engine.query(messages)
    print(response)
    return response
```

In addition, the backend exposes a `/getDataSource` endpoint that can be used to retrieve the original PDF file for a given source reference. You can find the full code in the `examples/llama-index/backend/main.py` file.

Note: There is an advanced example of a backend with highlighting in the `examples/llama-index/backend/main_with_highlighting.py` file.

## Frontend Description

The frontend is a React application configured with Vite. It provides GUI to interact with the backend and display query results together with the original PDF file and sources. The main components are:


- **Lexio UI Components**: The application uses `lexio` components only:
  - `RAGProvider` for providing the RAG context to the UI components.
  - `AdvancedQueryField` for inputting queries.
  - `ChatWindow` for displaying conversation history.
  - `SourcesDisplay` and `ContentDisplay` for showing retrieved documents and their content with highlighting.


For more details, check out the `examples/llama-index/frontend` directory, which contains the full source code.


